# Referenced from https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1
from keras import Sequential
from keras.layers import Dense, Flatten

for i in range(len(pen_types)):
    #Get Label new data
    label_data = pd.read_excel(path_label, sheet_name = pen_types[i])
    temp = label_data['MachineOutput'].values        
    machine_label = np.concatenate((temp[::num_pos], temp[1::num_pos], temp[2::num_pos], temp[3::num_pos])) #Arrange according to position
    
   #Get label old data
    label_name = path_label2 + pen_types[i] + '_Corrected.xlsx'
    label_data2 = pd.read_excel(label_name,sheetname="180905_Feder "+pen_types[i]+" mit Maschine",skipfooter=15)
    label_data2.columns = [np.tile(np.arange(5),6)]
    labels = (2-label_data2[1].values).flatten(order="F").tolist()
    machine_label2 = np.concatenate((labels[::num_pos], labels[1::num_pos], labels[2::num_pos], labels[3::num_pos]))
    
    #Get Feature
    row = machine_label.size
    row_old = machine_label2.size
    col = 16 + 16 + 8 #Number of features
    
    feature = np.zeros((row, col))
    feature_old = np.zeros((row_old,col))
    
    for j in range(num_pos):        
        file_name = path_feature + file_feature[num_pos * i + j]
        file_name_old = path_feature_old + file_feature_old[num_pos * i + j]
        
        data = pd.read_csv(file_name, skiprows=10, decimal=',', delimiter=';', index_col = 'Datum Uhrzeit').values
        data_old = pd.read_csv(file_name_old, skiprows=10, decimal=',', delimiter=';', index_col = 'Datum Uhrzeit').values
        
        feature[row / 4 * j:row / 4 * j + row/4, :col] = data[:row/4, :col]
        feature_old[row_old / 4 * j:row_old / 4 * j + row_old/4, :col] = data_old[:row_old/4, :col]
    
    #Classification
    X = np.concatenate((feature,feature_old))
    Y = np.concatenate((machine_label,machine_label2))

    y_train = [0 if x==2 else x for x in y_train]
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
    
    #Normalize data
    scaler = StandardScaler()  
    scaler.fit(X_train)

    X_train = scaler.transform(X_train)  
    X_test = scaler.transform(X_test)
    
    #NN model
    model = Sequential()
    neurons = 50
    #First Hidden Layer
    model.add(Dense(neurons, activation='tanh'))
    #Second  Hidden Layer
    model.add(Dense(neurons, activation='linear'))

    #Output Layer
    model.add(Dense(1, activation='sigmoid'))

    #Compiling the neural network
    model.compile(optimizer ='sgd',loss='binary_crossentropy', metrics =['accuracy'])
    
    print("Classification of Pen type: {}".format(str(pen_types[i]).upper()))
    #Fitting the data to the training dataset
    history = model.fit(X_train,y_train, validation_split=0.33, epochs=5)
    
    eval_model = model.evaluate(X_train, y_train)
    print("Loss of classifier : {}, and accuracy : {}".format(eval_model[0],eval_model[1]))

    #Confusion matrix of classifier
    y_pred = model.predict(X_test)
    y_pred =(y_pred>0.5)
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    
    #Accuracy Plot
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('Model accuracy of {} pen'.format(pen_types[i]))
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.grid()
    plt.show()
    
    #Error plot
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss of {} pen'.format(pen_types[i]))
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.grid()
    plt.show()